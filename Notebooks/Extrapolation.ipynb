{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrapolation Notebook - Waste Tax Modeling\n",
    "\n",
    "'''\n",
    "This is a shorter Jupyter notebook that will be used to hold my method, and processes for: extrapolating trends, \n",
    "normalizing income bins, and setting all datasets to the same base year for fair comparison. Past this point, most inference\n",
    "will be done manually in a final LaTeX document for submission. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential (log-linear) fit: log(W) = a + g*year\n",
      "  Continuous growth rate g: 0.018060  (~1.8060% per year)\n",
      "  Discrete growth rate (e^g-1): 1.8224% per year\n",
      "  R^2 (log scale): 0.9414\n",
      "  RMSE (log scale): 0.0805\n",
      "\n",
      "Linear (levels) fit: W = c + b*year\n",
      "  Slope b (units/year): 3178690.194221\n",
      "  R^2 (levels): 0.9661\n",
      "  RMSE (levels): 10627173.6109\n",
      "\n",
      "Projection comparison for 2023:\n",
      "  Linear projection (2023):       294959914.732\n",
      "  Exponential projection (2023):  318420402.138\n",
      "  Difference (exp - lin):         23460487.405\n",
      "  Percent diff vs linear:         7.95%\n",
      "Saved projections!\n"
     ]
    }
   ],
   "source": [
    "# Trend Extrapolation - Muni Waste Projections\n",
    "\n",
    "'''\n",
    "To extrapolate municipal solid waste beyond 2018, we'll estimate a log-linear trend using historical national MSW data from 1960–2018,\n",
    "which implies a stable annual growth rate consistent with population and activity scaling. Despite its marginally smaller R^2 value,\n",
    "relative to a linear model this exponential specification is used as the baseline projection through 2023. Linear trend extrapolation \n",
    "will be used as a robustness check, but the exponential model is preferred due to its economic interpretation and assumed stability over\n",
    "longer horizons. The projection is used solely to construct an aggregate baseline and does not embed distributional or behavioral assumptions.\n",
    "'''\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# ---- paths ----\n",
    "in_path = os.path.expanduser(\"~/Desktop/DESCO Case/Municipal Waste/Total_MSW_1960_2018.csv\")\n",
    "out_path = os.path.expanduser(\"~/Desktop/DESCO Case/Municipal Waste/Total_MSW_1960_2023_Projections.csv\")\n",
    "\n",
    "# ---- load ----\n",
    "df = pd.read_csv(in_path).sort_values(\"year\").reset_index(drop=True)\n",
    "\n",
    "# keep numeric years/values only\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "df[\"total_msw_generated\"] = pd.to_numeric(df[\"total_msw_generated\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"year\", \"total_msw_generated\"]).copy()\n",
    "df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "# ---- fit models on 1960–2018 (observed) ----\n",
    "X = df[\"year\"].values.reshape(-1, 1)\n",
    "y = df[\"total_msw_generated\"].values\n",
    "\n",
    "# Linear\n",
    "lin = LinearRegression().fit(X, y)\n",
    "y_hat_lin = lin.predict(X)\n",
    "r2_lin = r2_score(y, y_hat_lin)\n",
    "rmse_lin = np.sqrt(mean_squared_error(y, y_hat_lin))\n",
    "\n",
    "# Exponential \n",
    "# Guard: waste should be positive for log\n",
    "if (y <= 0).any():\n",
    "    raise ValueError(\"Found non-positive waste values; cannot take log for exponential fit.\")\n",
    "y_log = np.log(y)\n",
    "exp = LinearRegression().fit(X, y_log)\n",
    "y_log_hat = exp.predict(X)\n",
    "r2_exp_log = r2_score(y_log, y_log_hat)\n",
    "rmse_exp_log = np.sqrt(mean_squared_error(y_log, y_log_hat))\n",
    "\n",
    "a = exp.intercept_\n",
    "g = exp.coef_[0]                         \n",
    "disc_g = np.exp(g) - 1               \n",
    "\n",
    "# ---- build projection years ----\n",
    "last_year = int(df[\"year\"].max())\n",
    "proj_years = np.arange(last_year + 1, 2024)  \n",
    "X_proj = proj_years.reshape(-1, 1)\n",
    "\n",
    "# Predictions for observed & projected years\n",
    "all_years = np.concatenate([df[\"year\"].values, proj_years])\n",
    "X_all = all_years.reshape(-1, 1)\n",
    "\n",
    "# Linear predictions\n",
    "pred_lin_all = lin.predict(X_all)\n",
    "\n",
    "# Exponential predictions\n",
    "pred_exp_all = np.exp(a + g * all_years)\n",
    "\n",
    "# -------------------------------------- CLAUDE.AI PAST THIS POINT -------------------------------------------\n",
    "\n",
    "# ---- assemble output frame ----\n",
    "out = pd.DataFrame({\n",
    "    \"year\": all_years,\n",
    "    \"msw_observed\": np.concatenate([df[\"total_msw_generated\"].values, np.array([np.nan] * len(proj_years))]),\n",
    "    \"msw_linear\": pred_lin_all,\n",
    "    \"msw_exponential\": pred_exp_all,\n",
    "})\n",
    "\n",
    "# Differences (exp - linear)\n",
    "out[\"diff_exp_minus_linear\"] = out[\"msw_exponential\"] - out[\"msw_linear\"]\n",
    "out[\"pct_diff_exp_vs_linear\"] = out[\"diff_exp_minus_linear\"] / out[\"msw_linear\"]\n",
    "\n",
    "# Mark which rows are projections\n",
    "out[\"is_projection\"] = out[\"msw_observed\"].isna()\n",
    "\n",
    "# Save\n",
    "out.to_csv(out_path, index=False)\n",
    "\n",
    "# Print robustness stats + key comparison \n",
    "print(\"Exponential (log-linear) fit: log(W) = a + g*year\")\n",
    "print(f\"  Continuous growth rate g: {g:.6f}  (~{g*100:.4f}% per year)\")\n",
    "print(f\"  Discrete growth rate (e^g-1): {disc_g*100:.4f}% per year\")\n",
    "print(f\"  R^2 (log scale): {r2_exp_log:.4f}\")\n",
    "print(f\"  RMSE (log scale): {rmse_exp_log:.4f}\\n\")\n",
    "print(\"Linear (levels) fit: W = c + b*year\")\n",
    "print(f\"  Slope b (units/year): {lin.coef_[0]:.6f}\")\n",
    "print(f\"  R^2 (levels): {r2_lin:.4f}\")\n",
    "print(f\"  RMSE (levels): {rmse_lin:.4f}\\n\")\n",
    "\n",
    "# Compare projected 2023 values specifically\n",
    "row_2023 = out[out[\"year\"] == 2023].iloc[0]\n",
    "print(\"Projection comparison for 2023:\")\n",
    "print(f\"  Linear projection (2023):       {row_2023['msw_linear']:.3f}\")\n",
    "print(f\"  Exponential projection (2023):  {row_2023['msw_exponential']:.3f}\")\n",
    "print(f\"  Difference (exp - lin):         {row_2023['diff_exp_minus_linear']:.3f}\")\n",
    "print(f\"  Percent diff vs linear:         {row_2023['pct_diff_exp_vs_linear']*100:.2f}%\")\n",
    "print(f\"Saved projections!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] 2018 percent sum (rebinned): 100.00\n",
      "[CHECK] 2019 percent sum (rebinned): 99.90\n",
      "[CHECK] 2020 percent sum (rebinned): 100.00\n",
      "[CHECK] 2021 percent sum (rebinned): 100.00\n",
      "[CHECK] 2022 percent sum (rebinned): 100.00\n",
      "[CHECK] 2023 percent sum (rebinned): 100.00\n",
      "Saved normalized + merged dataset to: ~/Desktop/DESCO Case/Income_Expenditure_NormalizedBins_2018_2023.csv\n"
     ]
    }
   ],
   "source": [
    "# Income Bin Normalization - Doc-String/Methodology created by me ~ All code is CLAUDE.AI \n",
    "\n",
    "\"\"\"\n",
    "Distributional bin normalization + merge (ACS income distribution ↔ BLS expenditure bins)\n",
    "\n",
    "Methodology\n",
    "--------------------------------------------------\n",
    "We have two datasets with different income-bin definitions:\n",
    "\n",
    "1) ACS-style income distribution (household counts + percents) in wider bins:\n",
    "   e.g., (<$10k, $10–14.9k, $15–24.9k, $25–34.9k, $35–49.9k, $50–74.9k, ...)\n",
    "\n",
    "2) BLS/CEX-style average annual expenditures in (generally) narrower / shifted bins:\n",
    "   e.g., (<$15k, $15–29.9k, $30–39.9k, $40–49.9k, $50–69.9k, $70–99.9k, ...)\n",
    "\n",
    "To place both datasets on a common indexing, we'll re-bin ACS quantities onto the\n",
    "expenditure bin grid using an overlapping rule:\n",
    "\n",
    "- Each original ACS bin is treated as an income interval [L, U).\n",
    "- Each target bin is treated as an income interval [l, u).\n",
    "- For any target bin, we allocate a fraction of the source bin’s mass proportional\n",
    "  to the overlap length between intervals:\n",
    "      weight = overlap([L,U), [l,u)) / length([L,U))\n",
    "  This essentially equivalent to assuming a uniform distribution of households within each\n",
    "  source income bracket. This approach also should reduce some of the error associated with our\n",
    "  exponential growth assumptions as it should pull values closer to a true mean. \n",
    "\n",
    "  All code is Claude AI unless otherwise noted in a comment\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# INPUTS\n",
    "# -----------------------------\n",
    "acs_path = \"~/Desktop/DESCO Case/Income Data/Cleaned_Year_Wide.csv\" # Edited Paths\n",
    "cex_path = \"~/Desktop/DESCO Case/Expenditure by Income/Expenditure_By_Income_2018_2023.csv\"\n",
    "\n",
    "out_path = \"~/Desktop/DESCO Case/Income_Expenditure_NormalizedBins_2018_2023.csv\"\n",
    "\n",
    "acs = pd.read_csv(acs_path)\n",
    "cex = pd.read_csv(cex_path)\n",
    "\n",
    "acs_label_col = acs.columns[0]          # \"Label (Grouping)\"\n",
    "cex_label_col = cex.columns[0]          # \"income_bin\"\n",
    "\n",
    "# Keep only the ACS rows that are true income bins (drop totals/mean/median)\n",
    "drop_labels = {\n",
    "    \"Total households\",\n",
    "    \"Mean household income (dollars)\",\n",
    "    \"Median household income (dollars)\"\n",
    "}\n",
    "acs_bins = acs[~acs[acs_label_col].isin(drop_labels)].copy()\n",
    "\n",
    "# Standardize the “$200,000 or more” vs “$200,000 and more” label\n",
    "def normalize_bin_label(s: str) -> str:\n",
    "    s = str(s).replace(\"\\u00a0\", \" \").strip()\n",
    "    s = s.replace(\"or more\", \"and more\")\n",
    "    return s\n",
    "\n",
    "acs_bins[acs_label_col] = acs_bins[acs_label_col].map(normalize_bin_label)\n",
    "cex[cex_label_col] = cex[cex_label_col].map(normalize_bin_label)\n",
    "\n",
    "# Target bins will be the expenditure bins (rows of CEX file)\n",
    "target_bins = cex[cex_label_col].tolist()\n",
    "\n",
    "\n",
    "# PARSE BIN STRINGS -> INTERVALS\n",
    "def parse_income_bin_to_interval(label: str):\n",
    "    \"\"\"\n",
    "    Convert labels like:\n",
    "      \"Less than $15,000\"      -> (0, 15000)\n",
    "      \"$15,000 to $29,999\"     -> (15000, 30000)  [treat as half-open; +1 on upper not needed if we use overlap lengths]\n",
    "      \"$200,000 and more\"      -> (200000, np.inf)\n",
    "    Returns: (low, high) with high possibly np.inf\n",
    "    \"\"\"\n",
    "    s = normalize_bin_label(label)\n",
    "\n",
    "    # Less than\n",
    "    m = re.match(r\"Less than\\s*\\$(\\d[\\d,]*)\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        hi = int(m.group(1).replace(\",\", \"\"))\n",
    "        return (0.0, float(hi))\n",
    "\n",
    "    # Range\n",
    "    m = re.match(r\"\\$(\\d[\\d,]*)\\s*to\\s*\\$(\\d[\\d,]*)\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        lo = int(m.group(1).replace(\",\", \"\"))\n",
    "        hi = int(m.group(2).replace(\",\", \"\"))\n",
    "        # Treat \"$15,000 to $29,999\" as [15000, 30000) for clean overlap math\n",
    "        return (float(lo), float(hi + 1))\n",
    "\n",
    "    # Top-coded\n",
    "    m = re.match(r\"\\$(\\d[\\d,]*)\\s*(and more|or more)\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        lo = int(m.group(1).replace(\",\", \"\"))\n",
    "        return (float(lo), float(\"inf\"))\n",
    "\n",
    "    raise ValueError(f\"Could not parse income bin label: {label}\")\n",
    "\n",
    "def interval_overlap(a, b):\n",
    "    \"\"\"Overlap length of intervals a=[L,U), b=[l,u).\"\"\"\n",
    "    L, U = a\n",
    "    l, u = b\n",
    "    left = max(L, l)\n",
    "    right = min(U, u)\n",
    "    if right <= left:\n",
    "        return 0.0\n",
    "    # If either bound is inf, overlap is handled by min/max above.\n",
    "    if np.isinf(right) and np.isinf(left):\n",
    "        return 0.0\n",
    "    if np.isinf(right):\n",
    "        # overlap from left to infinity: treat as infinite, but this only occurs for top bin mapping\n",
    "        return float(\"inf\")\n",
    "    return float(right - left)\n",
    "\n",
    "# Build target bin intervals\n",
    "target_intervals = {tb: parse_income_bin_to_interval(tb) for tb in target_bins}\n",
    "\n",
    "# Build source (ACS) bin intervals\n",
    "source_bins = acs_bins[acs_label_col].tolist()\n",
    "source_intervals = {sb: parse_income_bin_to_interval(sb) for sb in source_bins}\n",
    "\n",
    "# CLEAN ACS NUMERIC COLUMNS\n",
    "def to_numeric_series(x: pd.Series):\n",
    "    \"\"\"Convert strings like '5,507,051' or '4.6%' or '±15,243' -> float (NaN if not parseable).\"\"\"\n",
    "    s = x.astype(str).str.replace(\"\\u00a0\", \" \", regex=False).str.strip()\n",
    "    # strip ± and commas\n",
    "    s = s.str.replace(\"±\", \"\", regex=False).str.replace(\",\", \"\", regex=False)\n",
    "    # percent -> numeric percent value\n",
    "    is_pct = s.str.contains(\"%\", na=False)\n",
    "    s = s.str.replace(\"%\", \"\", regex=False)\n",
    "    out = pd.to_numeric(s, errors=\"coerce\")\n",
    "    # keep percent as percent (not fraction), e.g., 4.6 means 4.6%\n",
    "    return out\n",
    "\n",
    "# Identify yearly ACS columns for Estimate and Percent\n",
    "# Example: \"2018_United_States_Estimate\", \"2018_United_States_Percent\"\n",
    "acs_cols = acs_bins.columns.tolist()\n",
    "yearly_est_cols = [c for c in acs_cols if re.search(r\"\\b\\d{4}.*Estimate\\b\", c)]\n",
    "yearly_pct_cols = [c for c in acs_cols if re.search(r\"\\b\\d{4}.*Percent\\b\", c) and \"Margin\" not in c]\n",
    "\n",
    "# Convert them to numeric\n",
    "for c in yearly_est_cols + yearly_pct_cols:\n",
    "    acs_bins[c] = to_numeric_series(acs_bins[c])\n",
    "\n",
    "# REBIN ACS -> TARGET BINS (by year, for Estimate + Percent)\n",
    "def rebin_vector(source_values_by_bin: dict, value_type: str):\n",
    "    \"\"\"\n",
    "    Re-bin a dict of {source_bin: value} onto target bins using overlap weights.\n",
    "    value_type:\n",
    "      - \"mass\": allocate additively (counts, totals)\n",
    "      - \"share\": allocate additively as well because ACS percent bins represent mass shares;\n",
    "                if the original shares sum to ~100, rebinned shares should also ~100.\n",
    "    Returns dict {target_bin: rebinned_value}\n",
    "    \"\"\"\n",
    "    out = {tb: 0.0 for tb in target_bins}\n",
    "\n",
    "    for sb, v in source_values_by_bin.items():\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "            continue\n",
    "\n",
    "        (L, U) = source_intervals[sb]\n",
    "\n",
    "        # Top-coded: assign entirely to the top target bin that overlaps\n",
    "        if np.isinf(U):\n",
    "            for tb in target_bins:\n",
    "                l, u = target_intervals[tb]\n",
    "                if np.isinf(u) and l == L:\n",
    "                    out[tb] += float(v)\n",
    "                    break\n",
    "            continue\n",
    "\n",
    "        length = U - L\n",
    "        if length <= 0:\n",
    "            continue\n",
    "\n",
    "        for tb in target_bins:\n",
    "            ov = interval_overlap((L, U), target_intervals[tb])\n",
    "            if ov <= 0:\n",
    "                continue\n",
    "            w = ov / length\n",
    "            out[tb] += float(v) * w\n",
    "\n",
    "    return out\n",
    "\n",
    "# Build a normalized frame indexed by target bins\n",
    "norm = pd.DataFrame({ \"income_bin_normalized\": target_bins })\n",
    "\n",
    "# For each year column, re-bin estimates and percents separately\n",
    "for c in yearly_est_cols:\n",
    "    year = re.search(r\"(\\d{4})\", c).group(1)\n",
    "    src = dict(zip(acs_bins[acs_label_col], acs_bins[c]))\n",
    "    reb = rebin_vector(src, value_type=\"mass\")\n",
    "    norm[f\"{year}_households_estimate\"] = norm[\"income_bin_normalized\"].map(reb)\n",
    "\n",
    "for c in yearly_pct_cols:\n",
    "    year = re.search(r\"(\\d{4})\", c).group(1)\n",
    "    src = dict(zip(acs_bins[acs_label_col], acs_bins[c]))\n",
    "    reb = rebin_vector(src, value_type=\"share\")\n",
    "    norm[f\"{year}_households_percent\"] = norm[\"income_bin_normalized\"].map(reb)\n",
    "\n",
    "# MERGE IN EXPENDITURE (already on target bins)\n",
    "# Ensure CEX is keyed on the same normalized label\n",
    "cex2 = cex.rename(columns={cex_label_col: \"income_bin_normalized\"}).copy()\n",
    "\n",
    "# Merge (left keeps target bin ordering)\n",
    "merged = norm.merge(cex2, on=\"income_bin_normalized\", how=\"left\")\n",
    "\n",
    "# Optional: sort columns for readability\n",
    "# Put key first, then ACS households, then expenditure columns\n",
    "key = [\"income_bin_normalized\"]\n",
    "acs_house_cols = sorted([c for c in merged.columns if \"households_\" in c])\n",
    "exp_cols = sorted([c for c in merged.columns if c.endswith(\"_average_expenditure\")])\n",
    "other = [c for c in merged.columns if c not in set(key + acs_house_cols + exp_cols)]\n",
    "\n",
    "merged = merged[key + acs_house_cols + exp_cols + other]\n",
    "\n",
    "# LIGHT SANITY CHECKS (printed, not required)\n",
    "# Shares should be ~100 each year (small deviations from rounding are normal)\n",
    "for yr in sorted(set(re.search(r\"(\\d{4})\", c).group(1) for c in acs_house_cols)):\n",
    "    pct_col = f\"{yr}_households_percent\"\n",
    "    if pct_col in merged.columns:\n",
    "        s = merged[pct_col].sum(skipna=True)\n",
    "        print(f\"[CHECK] {yr} percent sum (rebinned): {s:.2f}\")\n",
    "\n",
    "# SAVE\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(f\"Saved normalized + merged dataset to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
